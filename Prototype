import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import gym
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import zscore  # For normalization

# Toy Policy Network (MLP for CartPole)
class Policy(nn.Module):
    def __init__(self, obs_dim, act_dim):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(obs_dim, 64), nn.ReLU(), nn.Linear(64, act_dim), nn.Softmax(dim=-1))
    
    def forward(self, x):
        return self.net(x)

# Simple PPO Update (clipped objective, no value func for brevity)
def ppo_update(policy, optimizer, states, actions, old_probs, advantages, clip=0.2, epochs=4):
    for _ in range(epochs):
        probs = policy(states)
        ratios = probs[range(len(actions)), actions] / old_probs[range(len(actions)), actions]
        surr1 = ratios * advantages
        surr2 = torch.clamp(ratios, 1-clip, 1+clip) * advantages
        loss = -torch.min(surr1, surr2).mean()
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    return policy

# TRP Time Control Module
class TRPControl:
    def __init__(self, dt_band=(0.5, 2.0), kl_threshold=0.02):
        self.dt_band = dt_band
        self.kl_threshold = kl_threshold
    
    def compute_dt_eff(self, policy, rewards, states):
        # z(P_t): Normalized policy entropy
        with torch.no_grad():
            probs = policy(states)
            entropy = Categorical(probs).entropy().mean().item()
            z_p = zscore(np.array([entropy]))[0]  # Toy z-score (single point for demo)
        
        # z(R_t): Normalized reward std
        z_r = zscore(rewards)[0] if len(rewards) > 1 else 0
        
        dt_eff = z_p * z_r
        # Clamp to dt band
        dt_eff = np.clip(dt_eff, *self.dt_band)
        
        # KL-leash: Check policy divergence (toy: entropy drop)
        kl = abs(entropy - np.mean([1.0]))  # Placeholder baseline entropy=1
        if kl > self.kl_threshold:
            dt_eff *= 0.5  # Throttle
        
        return max(dt_eff, 0.1)  # Min step

# Training Loop (with/without TRP)
def train_ppo(env_name='CartPole-v1', use_trp=False, seeds=1, episodes=100):
    env = gym.make(env_name)
    obs_dim = env.observation_space.shape[0]
    act_dim = env.action_space.n
    scores = np.zeros((seeds, episodes))
    
    for seed in range(seeds):
        torch.manual_seed(seed)
        env.seed(seed)
        policy = Policy(obs_dim, act_dim)
        optimizer = optim.Adam(policy.parameters(), lr=3e-4)
        trp = TRPControl() if use_trp else None
        
        state = env.reset()
        ep_score, ep_states, ep_actions, ep_rewards, ep_old_probs = 0, [], [], [], []
        
        for ep in range(episodes):
            state = env.reset() if ep % 10 == 0 else state  # Batch episodes
            ep_score = 0
            batch_states, batch_actions, batch_probs, batch_rews = [], [], [], []
            
            for t in range(200):  # Max steps
                state_t = torch.FloatTensor([state]).requires_grad_(True)
                probs = policy(state_t)
                m = Categorical(probs)
                action = m.sample()
                old_prob = probs[0, action].detach()
                
                next_state, rew, done, _ = env.step(action.item())
                ep_score += rew
                
                batch_states.append(state_t.detach())
                batch_actions.append(action.item())
                batch_probs.append(old_prob)
                batch_rews.append(rew)
                
                state = next_state
                if done: break
            
            # TRP scaling: Adjust effective steps (toy: scale advantages)
            advantages = np.array(batch_rews) - np.mean(batch_rews)
            if use_trp and len(batch_states) > 1:
                dt_eff = trp.compute_dt_eff(policy, batch_rews, torch.cat(batch_states))
                advantages *= dt_eff  # Scale adv for "time control"
            
            # Update
            states_t = torch.cat(batch_states)
            policy = ppo_update(policy, optimizer, states_t, batch_actions, torch.stack(batch_probs), torch.FloatTensor(advantages))
            
            scores[seed, ep] = ep_score
            if ep % 10 == 0: print(f"Seed {seed}, Ep {ep}: Score {ep_score:.1f}, dt_eff {dt_eff if use_trp else 'N/A'}")
    
    # AULC: Trapezoidal integral of mean score curve
    mean_scores = scores.mean(0)
    aulc = np.trapz(mean_scores, dx=1)
    return scores, aulc

# Run Baselines
baseline_scores, baseline_aulc = train_ppo(use_trp=False)
trp_scores, trp_aulc = train_ppo(use_trp=True)

print(f"Baseline AULC: {baseline_aulc:.2f}")
print(f"TRP AULC: {trp_aulc:.2f}")
print(f"Improvement: {((trp_aulc - baseline_aulc) / baseline_aulc * 100):.1f}%")

# Plot (simulated output description; in REPL, this would save fig)
# plt.plot(np.cumsum(baseline_scores.mean(0)), label='Vanilla PPO')
# plt.plot(np.cumsum(trp_scores.mean(0)), label='TRP-PPO')
# plt.legend(); plt.show()
